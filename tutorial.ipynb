{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84798d4",
   "metadata": {},
   "source": [
    "# Tutorial: The Symbolic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sympy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from model.transformer_model import TransformerModel\n",
    "from model._utils import count_nb_params\n",
    "from model._utils import is_tree_complete\n",
    "from model._utils import translate_integers_into_tokens\n",
    "from datasets._utils import from_sequence_to_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971151b5",
   "metadata": {},
   "source": [
    "### 1. Instantiate the Symbolic Transformer\n",
    "\n",
    "We begin by instantiating an empty Symbolic Transformer.  \n",
    "Count and print its number of total trainable parameters (for information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ed11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First reload big model\n",
    "symbolic_transformer = TransformerModel(\n",
    "    enc_type='mix',\n",
    "    nb_samples=50,  # Number of samples par dataset\n",
    "    max_nb_var=7,  # Max number of variables\n",
    "    d_model=256,\n",
    "    vocab_size=18+2,  # len(vocab) + padding token + <SOS> token\n",
    "    seq_length=30,  # vocab_size + 1 - 1 (add <SOS> but shifted right)\n",
    "    h=4,\n",
    "    N_enc=4,\n",
    "    N_dec=8,\n",
    "    dropout=0.25,\n",
    ")\n",
    "total_nb_params = count_nb_params(symbolic_transformer, print_all=False)\n",
    "print(f'Total number params = {total_nb_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adfe11",
   "metadata": {},
   "source": [
    "### 2. Load pre-trained state, using `OrderedDict` to match module names exactly\n",
    "\n",
    "Specify the path with the best Symbolic Transformer model's weights.  \n",
    "Load the weights, and remove `module.` from their name to match the new names.  \n",
    "Substitute the weights from the empty Symbolic Transformer model to the best weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac44284",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_WEIGHTS = 'best_model_weights/mix_label_smoothing/model_weights.pt'\n",
    "hixon_state_dict = torch.load(PATH_WEIGHTS, map_location=torch.device('cpu'))\n",
    "\n",
    "my_state_dict = OrderedDict()\n",
    "for key in hixon_state_dict.keys():\n",
    "    assert key[:7]==\"module.\"\n",
    "    my_state_dict[key[7:]] = hixon_state_dict[key]\n",
    "\n",
    "out = symbolic_transformer.load_state_dict(my_state_dict, strict=True)\n",
    "symbolic_transformer.eval()  # deactivate training mode (important)\n",
    "print(out)  # This should print <All keys matched susccessfully>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d332d5",
   "metadata": {},
   "source": [
    "### 3. A function to decode in an auto-regressive fashion\n",
    "\n",
    "This function will be used to decode with the Symbolic Transformer in an auto-regression fashion.  \n",
    "Start by feeding the numerical tabular dataset to the Encoder. The `encoder_output` is the same for the whole procedure.  \n",
    "Initiate the Decoder with the start of sequence `<SOS>` token in first position.  \n",
    "Then loop: countinue decoding until the equation tree is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08478cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_with_symbolic_transformer(symbolic_transformer, dataset):\n",
    "    \"\"\"\n",
    "    Greedy decode with the Symbolic Transformer.\n",
    "    Decode until the equation tree is completed.\n",
    "    Parameters:\n",
    "      - symbolic_transformer: torch Module object\n",
    "      - dataset: tabular dataset\n",
    "      shape = (batch_size=1, nb_samples=50, nb_max_var=7, 1)\n",
    "    \"\"\"\n",
    "    encoder_output = symbolic_transformer.encoder(dataset)  # Encoder output is fixed for the batch\n",
    "    seq_length = symbolic_transformer.decoder.positional_encoding.seq_length\n",
    "    decoder_output = torch.zeros((dataset.shape[0], seq_length+1), dtype=torch.int64)  # initialize Decoder output\n",
    "    decoder_output[:, 0] = 1\n",
    "    is_complete = torch.zeros(dataset.shape[0], dtype=torch.bool)  # check when decoding is finished\n",
    "    \n",
    "    for n1 in range(seq_length):\n",
    "        padding_mask = torch.eq(decoder_output[:, :-1], 0).unsqueeze(1).unsqueeze(1)\n",
    "        future_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "        mask_dec = torch.logical_or(padding_mask, future_mask)\n",
    "        temp = symbolic_transformer.decoder(\n",
    "            target_seq=decoder_output[:, :-1],\n",
    "            mask_dec=mask_dec,\n",
    "            output_enc=encoder_output,\n",
    "        )\n",
    "        temp = symbolic_transformer.last_layer(temp)\n",
    "        decoder_output[:, n1+1] = torch.where(is_complete, 0, torch.argmax(temp[:, n1], axis=-1))\n",
    "        for n2 in range(dataset.shape[0]):\n",
    "            if is_tree_complete(decoder_output[n2, 1:]):\n",
    "                is_complete[n2] = True\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50429b5b",
   "metadata": {},
   "source": [
    "### 4. Generate your own tabular dataset\n",
    "\n",
    "Instantiate the necessary SymPy symbols.  \n",
    "Design custom ground-truth equation, and print the ground-truth LATEX formula.  \n",
    "Sample numerical values, and create a random tabular `dataset` following the ground-truth equation.  \n",
    "Generate `encoder_input` and feed it to the Symbolic Transformer using the above decoding function.  \n",
    "Decode the predict sequence of tokens into SymPy equation, and print it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SymPy symbols\n",
    "C, y, x1, x2, x3, x4, x5, x6 = sympy.symbols('C, y, x1, x2, x3, x4, x5, x6', real=True, positive=True)\n",
    "\n",
    "# Create your own ground truth\n",
    "y = 25 * x1 + x2 * sympy.log(x1)\n",
    "print('The ground truth is:')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample numerical values for x1 and x2 (add more columns if necessary, otherwise zeros)\n",
    "x1_values = np.power(10.0, np.random.uniform(-1.0, 1.0, size=50))\n",
    "x2_values = np.power(10.0, np.random.uniform(-1.0, 1.0, size=50))\n",
    "\n",
    "# Evaluate the ground_truth\n",
    "f = sympy.lambdify([x1, x2], y)\n",
    "y_values = f(x1_values, x2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724566ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tabular dataset\n",
    "dataset = np.zeros((50, 7))\n",
    "dataset[:, 0] = y_values\n",
    "dataset[:, 1] = x1_values\n",
    "dataset[:, 2] = x2_values\n",
    "\n",
    "# Generate input for the Encoder using torch.Tensor object\n",
    "encoder_input = torch.Tensor(dataset).unsqueeze(0).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6301214",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = decode_with_symbolic_transformer(symbolic_transformer, encoder_input)\n",
    "decoder_tokens = translate_integers_into_tokens(decoder_output[0])\n",
    "sympy_pred = from_sequence_to_sympy(decoder_tokens)\n",
    "sympy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786d7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21969121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd96a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447aefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789bdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e6c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
